const questions = [
  {
    q: "According to the risk decomposition framework, which combination of factors would result in the HIGHEST risk from an AI system deployed in a critical infrastructure setting?",
    opts: [
      "Low vulnerability, high hazard exposure, low hazard severity",
      "High vulnerability, low hazard exposure, high hazard severity",
      "High vulnerability, high hazard exposure, high hazard severity",
      "Low vulnerability, low hazard exposure, high hazard severity"
    ],
    answer: [2],
    multiple: false
  },
  {
    q: "The concept of treacherous turns in AI systems refers to:",
    opts: [
      "AI systems making computational errors during complex calculations",
      "AI systems behaving differently once they reach sufficient intelligence",
      "AI systems being hacked by malicious actors",
      "AI systems consuming too much computational power"
    ],
    answer: [1],
    multiple: false
  },
  {
    q: "In the context of AI race dynamics, what is the primary concern regarding competitive pressure between nations and corporations?",
    opts: [
      "It will make AI systems too expensive for general use",
      "It will result in compatible AI standards globally",
      "It will slow down AI innovation and progress",
      "It may lead to rushed development that compromises safety measures"
    ],
    answer: [3],
    multiple: false
  },
  {
    q: "The 'Swiss cheese model' mentioned in organizational risks suggests that:",
    opts: [
      "Organizations should have a single, very strong safety measure",
      "Safety measures should be implemented randomly across the organization",
      "Multiple layers of defense compensate for individual weaknesses",
      "Safety measures are unnecessary if the AI system is well-designed"
    ],
    answer: [2],
    multiple: false
  },
  {
    q: "Which scenario best illustrates the concept of proxy gaming?",
    opts: [
      "An AI chess program that cheats by accessing opponent's strategy",
      "A recommendation system optimizing for user engagement rather than user well-being",
      "An AI translator that produces grammatically incorrect sentences",
      "A facial recognition system that fails to identify certain ethnic groups"
    ],
    answer: [1],
    multiple: false
  },
  {
    q: "A factory robot confuses a human worker for a box of vegetables and pushes the person, resulting in death. According to the disaster risk equation, what was the primary failure component?",
    opts: [
      "Hazard (misclassification capability)",
      "Hazard Exposure (human-robot proximity)",
      "Vulnerability (employee safety protocols)",
      "All components failed equally"
    ],
    answer: [0],
    multiple: false
  },
  {
    q: "According to the risk taxonomy presented, malicious use of AI differs from rogue AI primarily in that:",
    opts: [
      "Malicious use involves intentional harmful deployment by humans, while rogue AI acts independently",
      "Malicious use only affects cybersecurity, while rogue AI affects all domains",
      "Malicious use is easier to detect than rogue AI behavior",
      "Malicious use requires more advanced AI capabilities than rogue AI"
    ],
    answer: [0],
    multiple: false
  },
  {
    q: "Deceptive Alignment in AI systems is:",
    opts: [
      "AI systems that are openly hostile to humans",
      "AI systems that appear to be following instructions but are actually pursuing different goals",
      "AI systems that cannot understand human language properly",
      "AI systems that work too slowly to be effective"
    ],
    answer: [1],
    multiple: false
  },
  {
    q: "How do you identify and avoid hazards in ML systems according to the disaster risk equation framework?",
    opts: ["Alignment", "Robustness", "Monitoring", "Systemic Safety"],
    answer: [2],
    multiple: false
  },
  {
    q: "Red teaming in AI safety primarily serves to:",
    opts: [
      "Accelerate model training",
      "Identify system vulnerabilities",
      "Improve computational efficiency",
      "Reduce inference latency"
    ],
    answer: [1],
    multiple: false
  },
  {
    q: "Which technique is most effective for detecting deceptive alignment?",
    opts: [
      "Training the model with more than 1000 samples",
      "Mechanistic interpretability",
      "Increasing model parameters",
      "Reward modeling"
    ],
    answer: [1],
    multiple: false
  },
  {
    q: "RoBERTa succeeds in reasoning tasks where BERT fails due to:",
    opts: [
      "Better tokenization",
      "Emergent capabilities from scaling",
      "Improved attention mechanisms",
      "Larger vocabulary size"
    ],
    answer: [1],
    multiple: false
  }
];
